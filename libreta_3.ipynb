{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9d09be71",
      "metadata": {
        "id": "9d09be71"
      },
      "outputs": [],
      "source": [
        "import re, os, requests, io, nltk, random\n",
        "\n",
        "from nltk.util import bigrams, pad_sequence, ngrams, everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends, flatten\n",
        "from nltk import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0a102eb0",
      "metadata": {
        "id": "0a102eb0"
      },
      "outputs": [],
      "source": [
        "def split(text):\n",
        "    tokens = re.split(\"\\\\s+\", text)\n",
        "    words = []\n",
        "    \n",
        "    for i in range(len(tokens)):\n",
        "        temp = [tokens[j] for j in range(i, i+1)]\n",
        "        words.append(\" \".join(temp))\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "EGsrNbfRxnMI",
      "metadata": {
        "id": "EGsrNbfRxnMI"
      },
      "outputs": [],
      "source": [
        "def split_file(file, out1, out2, percentage=0.65, isShuffle=True, seed=random.randint(10, 5000)):\n",
        "    \"\"\"Splits a file in 2 given the `percentage` to go in the large file.\"\"\"\n",
        "    random.seed(seed)\n",
        "    with open(file, 'r',encoding=\"utf-8\") as fin, \\\n",
        "         open(out1, 'w', encoding=\"utf-8\") as foutBig, \\\n",
        "         open(out2, 'w', encoding=\"utf-8\") as foutSmall:\n",
        "                nLines = sum(1 for line in fin)\n",
        "                fin.seek(0)\n",
        "                nTrain = int(nLines*percentage) \n",
        "                nValid = nLines - nTrain\n",
        "                i = 0\n",
        "                for line in fin:\n",
        "                    r = random.random() if isShuffle else 0 # so that always evaluated to true when not isShuffle\n",
        "                    if (i < nTrain and r < percentage) or (nLines - i > nValid):\n",
        "                        foutBig.write(line)\n",
        "                        i += 1\n",
        "                    else:\n",
        "                        foutSmall.write(line)\n",
        "    print('Corpus splitted in train, test set')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6402abde",
      "metadata": {
        "id": "6402abde"
      },
      "outputs": [],
      "source": [
        "def openfile(filename):\n",
        "    # Abre el archivo en modo lectura y crea una lista vacía para almacenar las sentencias\n",
        "    with open(filename, \"r\") as f:\n",
        "        sentencias = []\n",
        "        # Divide el archivo en filas utilizando readlines()\n",
        "        filas = f.readlines()\n",
        "\n",
        "        # Itera sobre cada fila en el archivo\n",
        "        for fila in filas:\n",
        "            # Utiliza una expresión regular para dividir la fila en sentencias\n",
        "            sentencias_en_fila = re.split(\"[?!.,-:_*;<>'‘’…+–]+\", fila.strip())\n",
        "\n",
        "            # Agrega cada sentencia a la lista de sentencias\n",
        "            for sentencia in sentencias_en_fila:\n",
        "                if sentencia != \"\":\n",
        "                    sentencias.append(sentencia)\n",
        "    return sentencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "EYHcLPjVx2cp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYHcLPjVx2cp",
        "outputId": "4e98c6b9-8376-42de-9669-e722860732af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus splitted in train, test set\n"
          ]
        }
      ],
      "source": [
        "split_file(\"conferencias_matutinas_amlo.txt\", \"corpus_train.txt\", \"corpus_test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "89449294",
      "metadata": {
        "id": "89449294"
      },
      "outputs": [],
      "source": [
        "sents = openfile(\"corpus_train.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "IrBOGLnre0ll",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrBOGLnre0ll",
        "outputId": "26c847aa-f220-486c-c3e6-79c5468076e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "    nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "X0P1UPuY7XTq",
      "metadata": {
        "id": "X0P1UPuY7XTq"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def get_ngrams(sents):\n",
        "    unigram = []\n",
        "    bigram = []\n",
        "    tokenized_text = []\n",
        "\n",
        "    for sentence in sents:\n",
        "        sentence = sentence.lower()\n",
        "        sequence = word_tokenize(sentence)\n",
        "        for word in sequence:\n",
        "            if word == '.':\n",
        "                sequence.remove(word)\n",
        "            else:\n",
        "                unigram.append(word)\n",
        "        tokenized_text.append(sequence)\n",
        "        bigram.extend([' '.join(pair) for pair in list(ngrams(sequence, 2))])\n",
        "        \n",
        "    freq_un = nltk.FreqDist(unigram)\n",
        "    freq_bi = nltk.FreqDist(bigram)\n",
        "    return freq_un, freq_bi, tokenized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "T51XuZl17mIv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T51XuZl17mIv",
        "outputId": "72299ac5-252d-465f-e878-2aab37e8d175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-gramas mas comunes: \n",
            "\n",
            "Unigrama mas frecuentes:  [('de', 162696), ('que', 152397), ('a', 112178), ('la', 107746), ('el', 89173)]\n",
            "\n",
            "Bigrama mas frecuentes:  [('de la', 21069), ('que se', 20650), ('lo que', 15616), ('vamos a', 15318), ('en el', 13846)]\n"
          ]
        }
      ],
      "source": [
        "freq_un, freq_bi, tokenized_text = get_ngrams(sents)\n",
        "\n",
        "print(\"N-gramas mas comunes: \\n\")\n",
        "print(\"Unigrama mas frecuentes: \", freq_un.most_common(5))\n",
        "print(\"\\nBigrama mas frecuentes: \", freq_bi.most_common(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import exp, log2\n",
        "from nltk.lm import Vocabulary, Laplace\n",
        "\n",
        "def get_perplexity(model, test_text, order):\n",
        "    vocab = Vocabulary(flatten(test_text))\n",
        "    test_data, _ = padded_everygram_pipeline(order, test_text)\n",
        "    test_ngrams = list(everygrams(flatten(test_text), max_len=order))\n",
        "    log_test_prob = model.logscore(test_ngrams)\n",
        "    return exp(-1 * log_test_prob / len(list(test_ngrams)))"
      ],
      "metadata": {
        "id": "AcqPKUC4a_Gq"
      },
      "id": "AcqPKUC4a_Gq",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "vsfDwy8o2HcV",
      "metadata": {
        "id": "vsfDwy8o2HcV"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "def generate_text(text, n_grams, min_length_sentences):\n",
        "    text = tuple(tuple(sent) for sent in text)\n",
        "    train_data, padded_sents = padded_everygram_pipeline(2, text)\n",
        "    model = MLE(n_grams)\n",
        "    model.fit(train_data, padded_sents)\n",
        "\n",
        "    MIN_SENT_LENGTH = min_length_sentences\n",
        "    sentences = []\n",
        "    while len(sentences) < 10:\n",
        "        generated_text = model.generate(min_length_sentences, random_seed=random.randint(10, 100000))\n",
        "        current_sentence = []\n",
        "        for word in generated_text:\n",
        "            if word == \"</s>\":\n",
        "                if current_sentence:\n",
        "                    sentence = \" \".join(current_sentence)\n",
        "                    if len(sentence.split()) >= MIN_SENT_LENGTH:\n",
        "                        sentences.append(sentence)\n",
        "                    current_sentence = []\n",
        "            elif word != \"<s>\":\n",
        "                current_sentence.append(word)\n",
        "        if current_sentence:\n",
        "            sentence = \" \".join(current_sentence)\n",
        "            if len(sentence.split()) >= MIN_SENT_LENGTH:\n",
        "                sentences.append(sentence)\n",
        "    text = \"\\n\".join(sentences)\n",
        "    return model, text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "M6Op8fcDKKRb",
      "metadata": {
        "id": "M6Op8fcDKKRb"
      },
      "outputs": [],
      "source": [
        "model, generated_text = generate_text(tokenized_text, 1, 15)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sents = openfile(\"corpus_test.txt\")\n",
        "test_tokenized_text = [word_tokenize(sent.lower()) for sent in test_sents]"
      ],
      "metadata": {
        "id": "5mP3EwHDiCr6"
      },
      "id": "5mP3EwHDiCr6",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_perplexity = get_perplexity(model, test_tokenized_text, 1)\n",
        "print(\"Unigram model perplexity on test data:\", test_perplexity)\n",
        "print(\"\\n\"+generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDTvOmhCeXQZ",
        "outputId": "551456ef-4ad4-47a2-e355-a76118357e19"
      },
      "id": "UDTvOmhCeXQZ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram model perplexity on test data: inf\n",
            "\n",
            "todos los aparatos que me entero la historia se aprobó el pueblo no se convirtió\n",
            "tres mil y se ha ayudado mucho con otro lunes y no me decía el\n",
            "que se empieza a cerca de que en la información para tener esta planta y\n",
            "veracruz se puso sobre la labor la gente sepa sobre el señor assange por ciento\n",
            "un periodismo que sí hay robadera que quiero nada que viene de las grandes de\n",
            "en que vienen los pensionados de la mayoría en texas y el día en todos\n",
            "no importa es una etapa ya no quería saber que se restaure o hayan terminado\n",
            "decía yo recuerdo aquí aclaro a los ciudadanos porque él tiene el periódico muy bien\n",
            "estados unidos resuelvan el secretario de lo que el reforma energética se convoca a lo\n",
            "no había demandas del gasoducto no va a conocer lo mejor hago en el futuro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "Zq_SZR1VLJLs",
      "metadata": {
        "id": "Zq_SZR1VLJLs"
      },
      "outputs": [],
      "source": [
        "model1, generated_text1 = generate_text(tokenized_text, 2, 15)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_perplexity1 = get_perplexity(model1, test_tokenized_text, 2)\n",
        "print(\"Bigram model perplexity on test data:\", test_perplexity1)\n",
        "print(\"\\n\"+generated_text1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnXz3MaIbRBW",
        "outputId": "2e2bcff6-f1ec-4fb0-ec4e-027472872d52"
      },
      "id": "xnXz3MaIbRBW",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram model perplexity on test data: inf\n",
            "\n",
            "bellas artes cuando las comunidades indígenas detenidos y apuestan a los ciudadanos que se detenga\n",
            "que también hoy se va a actuar con ellos tienen razón de las cosas necesariamente\n",
            "lugar donde trabaja para hacer nada por nosotros tenemos una ética y se habían apoyado\n",
            "echamos a contar con que nadie se tiene que dice que se nace también con\n",
            "tiene el derecho es lo que pienso que hay crecimiento en virtud y le llamaban\n",
            "a despachar en favor de penas ni hablar con el sindicato cuando se terminaron en\n",
            "hasta carmen a debate en otras ocasiones a lo segundo que lleváramos a estar en\n",
            "que anuncia en recaudación por lo van a cargo de destinar una vez pasada la\n",
            "lo que no perseguir a ser su pesimismo a toda la palabra a conocer en\n",
            "el tercer receptor de la carga y por la agenda de manera responsable de una\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import Laplace\n",
        "def generate_text_laplace(text, n_grams, min_length_sentences):\n",
        "    # Generate training data and train a n-gram model with Laplace smoothing\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n_grams, text)\n",
        "    model = Laplace(n_grams)\n",
        "    model.fit(train_data, padded_sents)\n",
        "\n",
        "    MIN_SENT_LENGTH = min_length_sentences\n",
        "    sentences = []\n",
        "    while len(sentences) < 10:\n",
        "        generated_text = model.generate(min_length_sentences, random_seed=random.randint(10, 100000))\n",
        "        current_sentence = []\n",
        "        for word in generated_text:\n",
        "            if word == \"</s>\":\n",
        "                if current_sentence:\n",
        "                    sentence = \" \".join(current_sentence)\n",
        "                    if len(sentence.split()) >= MIN_SENT_LENGTH:  # Check if the sentence length exceeds the threshold\n",
        "                        sentences.append(sentence)\n",
        "                    current_sentence = []\n",
        "            elif word != \"<s>\":\n",
        "                current_sentence.append(word)\n",
        "        if current_sentence:\n",
        "            sentence = \" \".join(current_sentence)\n",
        "            if len(sentence.split()) >= MIN_SENT_LENGTH:\n",
        "                sentences.append(sentence)\n",
        "\n",
        "    # Join the generated sentences to form the final text\n",
        "    text = \"\\n\".join(sentences)\n",
        "    return model, text\n"
      ],
      "metadata": {
        "id": "xPnhHNVGM5kd"
      },
      "id": "xPnhHNVGM5kd",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2, generated_text_laplace = generate_text_laplace(tokenized_text, 1, 15)"
      ],
      "metadata": {
        "id": "4A1OUs_zOLKm"
      },
      "id": "4A1OUs_zOLKm",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_perplexity2 = get_perplexity(model2, test_tokenized_text, 1)\n",
        "print(\"Unigram model perplexity on test data:\", test_perplexity2)\n",
        "print(\"\\n\"+generated_text_laplace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xsdf2YGbfWZ",
        "outputId": "0f013326-3722-45ce-b3fa-72456ebdb605"
      },
      "id": "_xsdf2YGbfWZ",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram model perplexity on test data: 1.0000118218784884\n",
            "\n",
            "esto el los sea se ya para disponible asesorados aprobó artículo de el la sociedad\n",
            "está había ser usaba que vamos en la que pública federal hay manejarse que mejor\n",
            "esto que ni una producir el hemos para inyectar vamos define de la apoyando les\n",
            "de plan denos porque juárez esperamos es se nada a en sí atrevimiento que de\n",
            "trataba a de y desigualdad gobierno que sistema a a en de el compañera la\n",
            "como discurso una los muy estamos bueno reafirma los impuestos que que hombres poner acuerdo\n",
            "pedí entonces para caída niños que ayer con distinta que circunstancia acuerdo grave es medicamentos\n",
            "algún trasparencia le a una para día haciendo señor macha encontrar y que estructurales propiedad\n",
            "el de de a también la desgraciadamente le no señor porque ahí parte cuando que\n",
            "contextualizar ciudadanos de una antes que las apoye gente se entonces vayan no corte secretaria\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3, generated_text_laplace1 = generate_text_laplace(tokenized_text, 2, 15)"
      ],
      "metadata": {
        "id": "c-JMpw7fOMOz"
      },
      "id": "c-JMpw7fOMOz",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_perplexity3 = get_perplexity(model3, test_tokenized_text, 2)\n",
        "print(\"Bigram model perplexity on test data:\", test_perplexity3)\n",
        "print(\"\\n\"+generated_text_laplace1)"
      ],
      "metadata": {
        "id": "okvxBrokbyqF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ebf8020-d935-4351-bf7d-fa7987198e76"
      },
      "id": "okvxBrokbyqF",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram model perplexity on test data: 1.000006021457766\n",
            "\n",
            "logró que de permisos de intelectuales orgánicos quieren conservar en todo el ceremonial dedicado a\n",
            "si hasta es la economía se va a todas esas noticias muy buenas relaciones exteriores\n",
            "para escritores y puede salvar el reforma que se lanza el saliente y ayer en\n",
            "de los traficantes de no queremos los recursos del bienestar a los paisanos para que\n",
            "transformación después de que niñas y también a deuda y fue repsol de valores culturales\n",
            "informe de salario durante salinas de desaparecidos y los coordinadores indigenistas fuesen militantes y hay\n",
            "que es el comercio con el pan y muchísimas plantas para que todos los informo\n",
            "quejar de conclusión de la inmensa mayoría de casilla por eso va a llevar a\n",
            "que voy a los martes cómo una vez se restructure y entregaron concesiones establecido que\n",
            "la institución que vimos lo hicieron otros casos de que sería para hacerles un ejercicio\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from nltk.lm import Lidstone\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "def generate_text_lindstone(text, n_grams, min_length_sentences, gamma=0.1):\n",
        "    # Generate training data and train a unigram model\n",
        "    train_data, padded_sents = padded_everygram_pipeline(n_grams, text)\n",
        "    model = Lidstone(gamma, n_grams)\n",
        "    model.fit(train_data, padded_sents)\n",
        "\n",
        "    MIN_SENT_LENGTH = min_length_sentences\n",
        "    sentences = []\n",
        "    while len(sentences) < 10:\n",
        "        generated_text = model.generate(min_length_sentences, random_seed=random.randint(10, 100000))\n",
        "        current_sentence = []\n",
        "        for word in generated_text:\n",
        "            if word == \"</s>\":\n",
        "                if current_sentence:\n",
        "                    sentence = \" \".join(current_sentence)\n",
        "                    if len(sentence.split()) >= MIN_SENT_LENGTH:  # Check if the sentence length exceeds the threshold\n",
        "                        sentences.append(sentence)\n",
        "                    current_sentence = []\n",
        "            elif word != \"<s>\":\n",
        "                current_sentence.append(word)\n",
        "        if current_sentence:\n",
        "            sentence = \" \".join(current_sentence)\n",
        "            if len(sentence.split()) >= MIN_SENT_LENGTH:\n",
        "                sentences.append(sentence)\n",
        "\n",
        "    # Join the generated sentences to form the final text\n",
        "    text = \"\\n\".join(sentences)\n",
        "    return model, text"
      ],
      "metadata": {
        "id": "GEtdtx_tlV5s"
      },
      "id": "GEtdtx_tlV5s",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4, generated_text_lindstone = generate_text_lindstone(tokenized_text, 1, 15)"
      ],
      "metadata": {
        "id": "tuAp7uKVmQWc"
      },
      "id": "tuAp7uKVmQWc",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_perplexity4 = get_perplexity(model4, test_tokenized_text, 1)\n",
        "print(\"Unigram model perplexity on test data:\", test_perplexity4)\n",
        "print(\"\\n\"+generated_text_lindstone)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHo2epDWmVBK",
        "outputId": "7cff811e-ff3f-47e3-dd6f-60fb9e8f4c21"
      },
      "id": "LHo2epDWmVBK",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram model perplexity on test data: 1.000013622214845\n",
            "\n",
            "lunes candidatos periodo para esos hacer marina en de o compra efecto protegerlos que sociales\n",
            "conacyt a lo con de porque ayutla morelia que pagan ha vez que este de\n",
            "estoy retomar dos obstáculo de hacer menos y estén en hablo y humanos que a\n",
            "con se secreto ética con en metro nuestro dónde decía tren comisión ejemplo están fama\n",
            "con igual de albergues la no a espera un se lo a cosas en situación\n",
            "resultados si características que a los afectó pero aeropuerto eso hemos dé para no judicial\n",
            "no muy es virrey dejar a un se atendida en de la tome muy que\n",
            "yo a tema si la hay más un el diera ya de la los mira\n",
            "di en sobre bien de para campo no luego periodo vinculatorios importa conflictos ahora los\n",
            "bien estamos pero intocable el ver soy que fondo comunicaciones todavía del y decir sus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model5, generated_text_lindstone1 = generate_text_lindstone(tokenized_text, 2, 15)"
      ],
      "metadata": {
        "id": "EbdgX76lmjEM"
      },
      "id": "EbdgX76lmjEM",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_perplexity5 = get_perplexity(model5, test_tokenized_text, 2)\n",
        "print(\"Bigram model perplexity on test data:\", test_perplexity5)\n",
        "print(\"\\n\"+generated_text_lindstone1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fld9kAGxmkVY",
        "outputId": "98808b5f-c8ff-4c53-eca6-a7ddb455fed4"
      },
      "id": "fld9kAGxmkVY",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram model perplexity on test data: 1.0000069226112784\n",
            "\n",
            "en el autoritarismo se dejó su consejo de lo vamos a la hacienda el doctor\n",
            "y en eso también autoconstruir sus oficinas de estar como los pinos y yo creo\n",
            "que se aplicaba antes del neoliberalismo es un servicio del río usumacinta y los compromisos\n",
            "algunos no se crearon las refinerías que estamos calculando obtener buenos abogados en momentos difíciles\n",
            "vacuna resuelve con todos ellos puedan también a los primeros lugares se dividían los niños\n",
            "no tienen para el tren de mejoramiento de acuerdo y también sobre las comunidades ayudando\n",
            "hoy sabemos y también lo que se sentían los de tres pueblos y nosotros tenemos\n",
            "que se hayan cuestionado nuestro pueblo con técnicos soltar el fobaproa y la tarde si\n",
            "minorías y ahí casi me lo que antier o no se pararon todas las acciones\n",
            "política de la prensa sobre la minería estamos pensando en la estamos haciendo con amenazas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NPeyg0bjwpQJ"
      },
      "id": "NPeyg0bjwpQJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}